# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ar8amrI4OvsxC4i9nQfeukYnFIBa_IRA
"""

import gdown
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# file_id = "1EFQlS9C4YGT5CNjD_NX_Q-HaiyLH4sjE"
file_id = "1QfzguI680h2Od7VXoPTypVr2h-DWwvkk"
# Construct the direct download URL
# https://drive.google.com/file/d/14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q/view?usp=sharing
download_url = f"https://drive.google.com/uc?id={file_id}"

# Download the file
output = "cnn_data.csv"
gdown.download(download_url, output, quiet=False)

# Read the CSV file
data = pd.read_csv(output)
data = data.dropna()

# X = data.drop(columns = ['label'])
# y = data['label']
X = data.drop(data.columns[0],axis = 1).drop(columns = ['encoded_part','extracted_part'])
y = data['extracted_part']

print(X.shape)
print(y.shape)

df = X

df['label'] = y

df

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from tqdm import tqdm # Import tqdm
import joblib

# Assuming your DataFrame is named 'df' and the target variable is 'label'
X = df.drop(columns=['label']).values  # Features as NumPy array
y = df['label'].values  # Target variable as NumPy array
#Encode the target variable if it contains strings
le = LabelEncoder()  # Create a LabelEncoder object
y = le.fit_transform(y)  # Transform the target variable
# Get the number of unique classes
num_classes = len(np.unique(y))
# Split data into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# Normalize Features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
joblib.dump(scaler, "scaler.pkl")
X_val = scaler.transform(X_val)

# Add bias term to X
X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]
X_val_bias = np.c_[np.ones(X_val.shape[0]), X_val]

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic Regression training function (same as before)
def train_logistic_regression(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)  # Initialize weights
    for epoch in range(epochs):
        z = X @ theta
        h = sigmoid(z)
        gradient = (X.T @ (h - y)) / m
        theta -= lr * gradient
    return theta

# Prediction Logic (from your last cell)
X_val = scaler.transform(X_val)
#X_val_bias = np.c_[np.ones(X_val.shape[0]), X_val]
all_theta=joblib.load("/content/trained_model.joblib")
y_pred_val = []
for i in range(X_val_bias.shape[0]):
    scores = [sigmoid(X_val_bias[i] @ theta) for theta in all_theta]
    predicted_class = np.argmax(scores)
    y_pred_val.append(predicted_class)
y_pred_val = np.array(y_pred_val)
accuracy = accuracy_score(y_val, y_pred_val)
print(f"Validation Accuracy: {accuracy*100}")