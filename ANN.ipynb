{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdCu9huR1wK6MvM8yj/TuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivekanandaMudelli/hand_drawn_sketch_recognition/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PIc_9Wv1W_o1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib  # For saving the model\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_id = \"1EFQlS9C4YGT5CNjD_NX_Q-HaiyLH4sjE\"\n",
        "file_id = \"14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q\"\n",
        "\n",
        "# Construct the direct download URL\n",
        "# https://drive.google.com/file/d/14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q/view?usp=sharing\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download the file\n",
        "output = \"data.csv\"  # Change filename as needed\n",
        "gdown.download(download_url, output, quiet=False)\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(output)\n",
        "data = data.dropna()\n",
        "\n",
        "# X = data.drop(columns = ['label'])\n",
        "# y = data['label']\n",
        "X = data.drop(data.columns[0],axis = 1).drop(columns = ['encoded_part','extracted_part'])\n",
        "y = data['encoded_part']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksg2DEPXXPAq",
        "outputId": "7d407588-9a61-48cc-bf6d-95b1be7a3d7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q\n",
            "From (redirected): https://drive.google.com/uc?id=14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q&confirm=t&uuid=08608c42-bf6f-4e0c-ab6b-25b8f9eabb89\n",
            "To: /content/data.csv\n",
            "100%|██████████| 793M/793M [00:11<00:00, 67.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 2048)\n",
            "(20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  return 1 / (1 + np.exp(-np.array(Z)))\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "  return sigmoid(Z) * (1 - sigmoid(Z))\n",
        "\n",
        "def ReLU(Z):\n",
        "  return np.maximum(0, np.array(Z))\n",
        "\n",
        "def ReLU_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # stability fix\n",
        "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "def linear(Z):\n",
        "  return Z\n",
        "\n",
        "def linear_derivative(Z):\n",
        "  return np.ones(Z.shape)\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    # Avoid log(0) by adding small value\n",
        "    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-9)\n",
        "    return np.sum(log_likelihood) / m\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))"
      ],
      "metadata": {
        "id": "3Q6Xw_9iXYCM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, activation='relu', learning_rate=0.01):\n",
        "        self.layers = layers\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(layers) - 1):\n",
        "            # Xavier Initialization\n",
        "            self.weights.append(np.random.randn(layers[i+1], layers[i]) * np.sqrt(1. / layers[i]))\n",
        "            self.biases.append(np.zeros((layers[i+1])))\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = [np.array(x)]\n",
        "        z_values = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(activations[-1], self.weights[i].T) + self.biases[i]\n",
        "            z_values.append(z)\n",
        "\n",
        "            if i == len(self.weights) - 1:\n",
        "                activations.append(softmax(z))  # Final layer softmax\n",
        "            elif self.activation == 'sigmoid':\n",
        "                activations.append(sigmoid(z))\n",
        "            elif self.activation == 'relu':\n",
        "                activations.append(ReLU(z))\n",
        "            elif self.activation == 'linear':\n",
        "                activations.append(linear(z))\n",
        "\n",
        "        return activations, z_values\n",
        "\n",
        "    def backward(self, x, y, activations, z_values):\n",
        "        m = y.shape[0] if y.ndim > 1 else 1\n",
        "        delta = activations[-1] - y  # Cross-entropy derivative\n",
        "\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            if i < len(self.weights) - 1:\n",
        "                if self.activation == 'sigmoid':\n",
        "                    delta *= sigmoid_derivative(z_values[i])\n",
        "                elif self.activation == 'relu':\n",
        "                    delta *= ReLU_derivative(z_values[i])\n",
        "                elif self.activation == 'linear':\n",
        "                    delta *= linear_derivative(z_values[i])\n",
        "\n",
        "            a_prev = activations[i] if activations[i].ndim > 1 else activations[i].reshape(1, -1)\n",
        "            delta_mat = delta if delta.ndim > 1 else delta.reshape(1, -1)\n",
        "\n",
        "            self.weights[i] -= self.learning_rate * (delta_mat.T @ a_prev) / m\n",
        "            self.biases[i] -= self.learning_rate * delta_mat.sum(axis=0) / m\n",
        "\n",
        "            delta = delta @ self.weights[i]\n",
        "\n",
        "    def train(self, x_train, y_train, x_val=None, y_val=None, epochs=20, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.arange(len(x_train))\n",
        "            np.random.shuffle(indices)\n",
        "            x_train, y_train = x_train[indices], y_train[indices]\n",
        "\n",
        "            for start in range(0, len(x_train), batch_size):\n",
        "                end = start + batch_size\n",
        "                x_batch = x_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "\n",
        "                activations, z_values = self.forward(x_batch)\n",
        "                self.backward(x_batch, y_batch, activations, z_values)\n",
        "\n",
        "            # Evaluate\n",
        "            train_pred = self.predict(x_train)\n",
        "            train_loss = cross_entropy_loss(y_train, train_pred)\n",
        "            train_acc = accuracy(y_train, train_pred)\n",
        "\n",
        "            if x_val is not None and y_val is not None:\n",
        "                val_pred = self.predict(x_val)\n",
        "                val_loss = cross_entropy_loss(y_val, val_pred)\n",
        "                val_acc = accuracy(y_val, val_pred)\n",
        "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        activations, _ = self.forward(x_test)\n",
        "        return activations[-1]\n"
      ],
      "metadata": {
        "id": "fhJB5eaGXofk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "y_onehot = np.eye(250)[y_encoded]  # One-hot encoding\n",
        "\n",
        "# Train/val/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "InlriZGtXwC1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Layers = np.array([X_train.shape[1],1024,512,250])\n",
        "ANN = NeuralNetwork(Layers,activation = 'relu',learning_rate=0.005)\n",
        "ANN.train(X_train,y_train, x_val=None, y_val=None, epochs=60, batch_size=64) # epochs=100, batch_size=64 (or any desired batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNXnX35-XznA",
        "outputId": "02af5bf6-00a0-4239-a3c8-baa5dab0c930"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60 | Train Loss: 5.0733 | Train Acc: 0.0852\n",
            "Epoch 2/60 | Train Loss: 4.4740 | Train Acc: 0.1875\n",
            "Epoch 3/60 | Train Loss: 3.8586 | Train Acc: 0.2768\n",
            "Epoch 4/60 | Train Loss: 3.3489 | Train Acc: 0.3473\n",
            "Epoch 5/60 | Train Loss: 2.9605 | Train Acc: 0.3985\n",
            "Epoch 6/60 | Train Loss: 2.6666 | Train Acc: 0.4439\n",
            "Epoch 7/60 | Train Loss: 2.4381 | Train Acc: 0.4749\n",
            "Epoch 8/60 | Train Loss: 2.2637 | Train Acc: 0.5013\n",
            "Epoch 9/60 | Train Loss: 2.1110 | Train Acc: 0.5257\n",
            "Epoch 10/60 | Train Loss: 1.9893 | Train Acc: 0.5463\n",
            "Epoch 11/60 | Train Loss: 1.8836 | Train Acc: 0.5637\n",
            "Epoch 12/60 | Train Loss: 1.7826 | Train Acc: 0.5859\n",
            "Epoch 13/60 | Train Loss: 1.7047 | Train Acc: 0.5979\n",
            "Epoch 14/60 | Train Loss: 1.6196 | Train Acc: 0.6190\n",
            "Epoch 15/60 | Train Loss: 1.5515 | Train Acc: 0.6354\n",
            "Epoch 16/60 | Train Loss: 1.4977 | Train Acc: 0.6418\n",
            "Epoch 17/60 | Train Loss: 1.4337 | Train Acc: 0.6567\n",
            "Epoch 18/60 | Train Loss: 1.3791 | Train Acc: 0.6694\n",
            "Epoch 19/60 | Train Loss: 1.3259 | Train Acc: 0.6813\n",
            "Epoch 20/60 | Train Loss: 1.2837 | Train Acc: 0.6921\n",
            "Epoch 21/60 | Train Loss: 1.2391 | Train Acc: 0.7024\n",
            "Epoch 22/60 | Train Loss: 1.1932 | Train Acc: 0.7117\n",
            "Epoch 23/60 | Train Loss: 1.1576 | Train Acc: 0.7199\n",
            "Epoch 24/60 | Train Loss: 1.1141 | Train Acc: 0.7356\n",
            "Epoch 25/60 | Train Loss: 1.0841 | Train Acc: 0.7378\n",
            "Epoch 26/60 | Train Loss: 1.0453 | Train Acc: 0.7508\n",
            "Epoch 27/60 | Train Loss: 1.0124 | Train Acc: 0.7567\n",
            "Epoch 28/60 | Train Loss: 0.9776 | Train Acc: 0.7712\n",
            "Epoch 29/60 | Train Loss: 0.9536 | Train Acc: 0.7748\n",
            "Epoch 30/60 | Train Loss: 0.9200 | Train Acc: 0.7867\n",
            "Epoch 31/60 | Train Loss: 0.9007 | Train Acc: 0.7900\n",
            "Epoch 32/60 | Train Loss: 0.8723 | Train Acc: 0.7971\n",
            "Epoch 33/60 | Train Loss: 0.8426 | Train Acc: 0.8053\n",
            "Epoch 34/60 | Train Loss: 0.8289 | Train Acc: 0.8057\n",
            "Epoch 35/60 | Train Loss: 0.8021 | Train Acc: 0.8107\n",
            "Epoch 36/60 | Train Loss: 0.7773 | Train Acc: 0.8174\n",
            "Epoch 37/60 | Train Loss: 0.7601 | Train Acc: 0.8229\n",
            "Epoch 38/60 | Train Loss: 0.7410 | Train Acc: 0.8306\n",
            "Epoch 39/60 | Train Loss: 0.7124 | Train Acc: 0.8383\n",
            "Epoch 40/60 | Train Loss: 0.6963 | Train Acc: 0.8406\n",
            "Epoch 41/60 | Train Loss: 0.6805 | Train Acc: 0.8462\n",
            "Epoch 42/60 | Train Loss: 0.6543 | Train Acc: 0.8555\n",
            "Epoch 43/60 | Train Loss: 0.6337 | Train Acc: 0.8585\n",
            "Epoch 44/60 | Train Loss: 0.6228 | Train Acc: 0.8602\n",
            "Epoch 45/60 | Train Loss: 0.6122 | Train Acc: 0.8628\n",
            "Epoch 46/60 | Train Loss: 0.5983 | Train Acc: 0.8672\n",
            "Epoch 47/60 | Train Loss: 0.5708 | Train Acc: 0.8749\n",
            "Epoch 48/60 | Train Loss: 0.5567 | Train Acc: 0.8768\n",
            "Epoch 49/60 | Train Loss: 0.5380 | Train Acc: 0.8831\n",
            "Epoch 50/60 | Train Loss: 0.5392 | Train Acc: 0.8844\n",
            "Epoch 51/60 | Train Loss: 0.5184 | Train Acc: 0.8891\n",
            "Epoch 52/60 | Train Loss: 0.4911 | Train Acc: 0.9010\n",
            "Epoch 53/60 | Train Loss: 0.4861 | Train Acc: 0.8984\n",
            "Epoch 54/60 | Train Loss: 0.4972 | Train Acc: 0.8926\n",
            "Epoch 55/60 | Train Loss: 0.4708 | Train Acc: 0.9018\n",
            "Epoch 56/60 | Train Loss: 0.4515 | Train Acc: 0.9058\n",
            "Epoch 57/60 | Train Loss: 0.4414 | Train Acc: 0.9084\n",
            "Epoch 58/60 | Train Loss: 0.4567 | Train Acc: 0.9005\n",
            "Epoch 59/60 | Train Loss: 0.4184 | Train Acc: 0.9161\n",
            "Epoch 60/60 | Train Loss: 0.4099 | Train Acc: 0.9181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ANN.predict(X_test)"
      ],
      "metadata": {
        "id": "ohIgVgFrX65i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
        "y_true_labels = np.argmax(y_test, axis=1)  # Convert one-hot to class labels\n",
        "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed performance metrics\n",
        "# print(\"\\nClassification Report:\")\n",
        "# print(classification_report(y_test, y_pred, target_names=label_encoder.classes_.astype(str)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfdHrL2YBlH",
        "outputId": "d48e89fc-870c-41a4-b1d9-b633a242d946"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5613\n"
          ]
        }
      ]
    }
  ]
}