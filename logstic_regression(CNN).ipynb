{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMVMxMcUtO4HcpJEZqf84x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivekanandaMudelli/hand_drawn_sketch_recognition/blob/main/logstic_regression(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DO3PjixVf59k"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_id = \"1EFQlS9C4YGT5CNjD_NX_Q-HaiyLH4sjE\"\n",
        "file_id = \"14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q\"\n",
        "\n",
        "# Construct the direct download URL\n",
        "# https://drive.google.com/file/d/14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q/view?usp=sharing\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download the file\n",
        "output = \"data.csv\"  # Change filename as needed\n",
        "gdown.download(download_url, output, quiet=False)\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(output)\n",
        "data = data.dropna()\n",
        "\n",
        "# X = data.drop(columns = ['label'])\n",
        "# y = data['label']\n",
        "X = data.drop(data.columns[0],axis = 1).drop(columns = ['encoded_part','extracted_part'])\n",
        "y = data['extracted_part']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNU3pbESgFZr",
        "outputId": "1eb0d379-e51c-4776-c5db-d213ee418f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q\n",
            "From (redirected): https://drive.google.com/uc?id=14X_8QdPW12hGHnNzZztuz3keHOo9Cq4q&confirm=t&uuid=91d44b2e-f47a-44c1-9690-dc36e094b388\n",
            "To: /content/data.csv\n",
            "100%|██████████| 793M/793M [00:09<00:00, 82.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean\n",
        "X_mean = X.mean()\n",
        "\n",
        "# Standard deviation\n",
        "X_std = X.std()\n",
        "\n",
        "# Standardization\n",
        "Z = (X - X_mean) / X_std.replace(0,1e-10)"
      ],
      "metadata": {
        "id": "O92fx51hgJwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Taking top 50 dimensions\n",
        "pca = PCA(n_components=500)\n",
        "pca.fit(Z)\n",
        "x_pca = pca.transform(Z)\n",
        "\n",
        "# Create the dataframe\n",
        "df = pd.DataFrame(x_pca,\n",
        "                       columns=['PC{}'.\n",
        "                       format(i+1)\n",
        "                        for i in range(500)])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "-432gy0sgMuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = X"
      ],
      "metadata": {
        "id": "7K8qN9plgQzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = y"
      ],
      "metadata": {
        "id": "9jSpf5FYgRpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BQRAzHGigVU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "D_tYytJ6gbiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm # Import tqdm"
      ],
      "metadata": {
        "id": "g4s_-gYigxA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataFrame is named 'df' and the target variable is 'label'\n",
        "X = df.drop(columns=['label']).values  # Features as NumPy array\n",
        "y = df['label'].values  # Target variable as NumPy array\n",
        "#Encode the target variable if it contains strings\n",
        "le = LabelEncoder()  # Create a LabelEncoder object\n",
        "y = le.fit_transform(y)  # Transform the target variable\n",
        "# Get the number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lHuwCo7hg6Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "TREWGmkig9Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add bias term to X\n",
        "X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "X_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]"
      ],
      "metadata": {
        "id": "l0YpaZEehD6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "tpSxqv_9hI-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression training function (same as before)\n",
        "def train_logistic_regression(X, y, lr=0.01, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)  # Initialize weights\n",
        "    for epoch in range(epochs):\n",
        "        z = X @ theta\n",
        "        h = sigmoid(z)\n",
        "        gradient = (X.T @ (h - y)) / m\n",
        "        theta -= lr * gradient\n",
        "        if epoch % 100 == 0:  # Print loss every 100 epochs\n",
        "            loss = -np.mean(y * np.log(h + 1e-9) + (1 - y) * np.log(1 - h + 1e-9))\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "    return theta"
      ],
      "metadata": {
        "id": "WIhjJdqhhPED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One-vs-Rest Training:\n",
        "all_theta = []  # To store the parameters for each class\n",
        "for i in tqdm(range(num_classes), desc=\"Training\"):  # Wrap range with tqdm\n",
        "    # Create a binary target variable for the current class\n",
        "    y_train_binary = (y_train == i).astype(int)\n",
        "    # Train a logistic regression model for this class\n",
        "    theta = train_logistic_regression(X_train_bias, y_train_binary, lr=0.1, epochs=1000)\n",
        "    all_theta.append(theta)"
      ],
      "metadata": {
        "id": "CtLFtjtYhQWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "y_pred_probs = []\n",
        "for i in range(num_classes):\n",
        "    z = X_test_bias @ all_theta[i]\n",
        "    y_pred_probs.append(sigmoid(z))\n",
        "# Get the class with the highest probability\n",
        "y_pred = np.argmax(y_pred_probs, axis=0)\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "D_57AXS_hbGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}